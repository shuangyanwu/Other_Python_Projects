{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Detect Similar Words Using the Word2Vec Technique - NLP Word Embeddings\n",
    "\n",
    "### Goal: find top 15 words that are similar to \"two\", \"america\", and \"computer\", respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f96f639f390>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "EUwq6AMMPGRs"
   },
   "outputs": [],
   "source": [
    "M = 3                # context window\n",
    "K = 15               # number of negative samples\n",
    "epochs = 2\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "EMBEDDING_SIZE = 100  \n",
    "batch_size = 32\n",
    "lr = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "2kAaifXXPIbt",
    "outputId": "9a479862-b9c4-4354-b64b-a04270ecb32a"
   },
   "outputs": [],
   "source": [
    "# open the file and read the texts\n",
    "with open('/Users/wushuangyan/Desktop/STAT classes/Data Science/text8.train.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text.lower().split()                                         # simple pre-processing\n",
    "vocab_dict = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1))    # extract a vocabulary, each vocabulary item is (word, word_index)\n",
    "vocab_dict['<UNK>'] = len(text) - np.sum(list(vocab_dict.values())) # encode the uncommon terms as \"<UNK>\"\n",
    "\n",
    "word2idx = {word:i for i, word in enumerate(vocab_dict.keys())}     # index\n",
    "idx2word = {i:word for i, word in enumerate(vocab_dict.keys())}\n",
    "word_counts = np.array([count for count in vocab_dict.values()], dtype=np.float32)\n",
    "word_freqs = word_counts / np.sum(word_counts)                      # freq or probability\n",
    "word_freqs = word_freqs ** (3./4.)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ba_M7RLPLXz"
   },
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(tud.Dataset):\n",
    "    def __init__(self, text, word2idx, word_freqs):\n",
    "        ''' text: a list of words, all text from the training dataset\n",
    "            word2idx: the dictionary from word to index\n",
    "            word_freqs: the frequency of each word\n",
    "        '''\n",
    "        super(WordEmbeddingDataset, self).__init__()                                    # next line: unknown works are given index of UNK's index\n",
    "        self.text_encoded = [word2idx.get(word, word2idx['<UNK>']) for word in text]    # map each word to its index\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded)                         # turn index to pytorch LongTensor type\n",
    "        self.word2idx = word2idx\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)                                                 # return the total number of words in the texts\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ''' Prepare training batches. Each mini-batch contains the following:\n",
    "            - idx: the index of the center word\n",
    "            - Get the neighbor words of the center word\n",
    "            - Randomly sample K words as negative sampling\n",
    "        '''\n",
    "        center_words = self.text_encoded[idx]                                         # center word by index\n",
    "        pos_indices = list(range(idx - M, idx)) + list(range(idx + 1, idx + M + 1))   # neighbor words\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]               # avoid index going beyond 0 or vocabulary size\n",
    "        pos_words = self.text_encoded[pos_indices]                                    # index for context words\n",
    "        \n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)  # 15 * 6 = 90, sampling freq for each context, negative sampling\n",
    "                                                                                      # show the position index\n",
    "                                                                                      # neg_words should not contain neighbor words\n",
    "        while len(set(pos_indices) & set(neg_words)) > 0:                             # pos =6 # neg =90, if contain, resampling\n",
    "            neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\n",
    "\n",
    "        return center_words, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wbh0crckPL1s"
   },
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataset(text, word2idx, word_freqs)\n",
    "dataloader = tud.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93wQkW_IPNVs"
   },
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):  # pos: Neignbor, neg: negative sampling \n",
    "        input_embedding = self.in_embed(input_labels)         # size:   [batch_size, EMBEDDING_SIZE]                            \n",
    "        pos_embedding = self.out_embed(pos_labels)            # size:   [batch_size, 2 * M * EMBEDDING_SIZE]\n",
    "        neg_embedding = self.out_embed(neg_labels)            # size:   [batch_size, 2 * M * K * EMBEDDING_SIZE]\n",
    "        \n",
    "        input_embedding = input_embedding.unsqueeze(2)        # size:   [batch_size, EMBEDDING_SIZE * 1]\n",
    "        \n",
    "        pos_dot = torch.bmm(pos_embedding, input_embedding)   # size:   [batch_size, 2*M * 1]\n",
    "        pos_dot = pos_dot.squeeze(2)                          # size:   [batch_size, 2*M]\n",
    "        \n",
    "        neg_dot = torch.bmm(neg_embedding, -input_embedding)  # size:   [batch_size, 2 * M * K * 1]\n",
    "        neg_dot = neg_dot.squeeze(2)                          # size:   [batch_size, 2 * M * K]\n",
    "        \n",
    "        log_pos = F.logsigmoid(pos_dot).sum(1)                # size:    batch_size\n",
    "        log_neg = F.logsigmoid(neg_dot).sum(1)              \n",
    "        \n",
    "        loss = log_pos + log_neg\n",
    "        \n",
    "        return -loss\n",
    "    \n",
    "    def input_embedding(self):\n",
    "        return self.in_embed.weight.detach().numpy()\n",
    "\n",
    "model = EmbeddingModel(MAX_VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLAzMfnmPQsj"
   },
   "outputs": [],
   "source": [
    "for e in range(1):\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        input_labels = input_labels.long()\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print('epoch', e, 'iteration', i, loss.item())\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            embedding_weights = model.input_embedding()\n",
    "\n",
    "embedding_weights = model.input_embedding()\n",
    "torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wfVdz0NOPSdD",
    "outputId": "979e4b3c-662a-42ab-b8d1-06bde1647197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two ['two', 'three', 'four', 'five', 'six', 'zero', 'one', 'seven', 'eight', 'nine', 'april', 'january', 'march', 'm', 'october']\n",
      "america ['america', 'europe', 'africa', 'asia', 'australia', 'south', 'north', 'western', 'african', 'india', 'southern', 'zealand', 'china', 'central', 'canada']\n",
      "computer ['computer', 'software', 'hardware', 'design', 'digital', 'computers', 'video', 'program', 'electronic', 'programs', 'technology', 'basic', 'programming', 'advanced', 'engineering']\n"
     ]
    }
   ],
   "source": [
    "def find_nearest(word):\n",
    "    index = word2idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "    return [idx2word[i] for i in cos_dis.argsort()[:15]]\n",
    "\n",
    "for word in [\"two\", \"america\", \"computer\"]:\n",
    "    print(word, find_nearest(word))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
