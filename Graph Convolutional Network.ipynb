{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1649777840414,
     "user": {
      "displayName": "NH L",
      "userId": "13973463595760031265"
     },
     "user_tz": 240
    },
    "id": "uLYQDNJjXn7c"
   },
   "source": [
    "# The Application of Graph Convolutional Network (GCN)\n",
    "\n",
    "### It is a method for semi-supervised learning on graph-structured data. It is based on an efficient variant of convolutional neural networks (CNNs) which operate directly on graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden = 16                                     # hidden dimension\n",
    "dropout = 0.5\n",
    "lr = 0.01 \n",
    "weight_decay = 5e-4\n",
    "fastmode = 'store_true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6014,
     "status": "ok",
     "timestamp": 1649777846723,
     "user": {
      "displayName": "NH L",
      "userId": "13973463595760031265"
     },
     "user_tz": 240
    },
    "id": "SFa73PGvX-bu",
    "outputId": "07ad9b1c-b03a-4a24-f4f0-9aee8e9b4dec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def  load_data(path=\"/Users/wushuangyan/Desktop/STAT classes/Data Science/HW4/\", dataset=\"cora\"):\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), # read node labels\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)  # read node features\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])                        # one-hot encoding for labels\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)                \n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),       # read edge information\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    features = normalize(features)                                            # feature normalization\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))                               # edge normalization\n",
    "\n",
    "    idx_train = range(140)                                                    # training set\n",
    "    idx_val = range(200, 500)                                                 # validation set\n",
    "    idx_test = range(500, 1500)                                               # testing set\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)                               # get adjacency matrix\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "                                                           \n",
    "    return adj, features, labels, idx_train, idx_val, idx_test          \n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1649777846724,
     "user": {
      "displayName": "NH L",
      "userId": "13973463595760031265"
     },
     "user_tz": 240
    },
    "id": "ObvNpbnwZ5gW"
   },
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):                            \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):             # AHW \n",
    "        support = torch.mm(input, self.weight) # (2708, 16) = (2708, 1433) X (1433, 16)\n",
    "        output = torch.spmm(adj, support)      # (2708, 16) = (2708, 2708) X (2708, 16)\n",
    "        return output + self.bias\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class GCN(nn.Module):                                             # a two-layer GCN\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = torch.nn.functional.relu(self.gc1(x, adj))\n",
    "        x = torch.nn.functional.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return torch.nn.functional.log_softmax(x, dim=1)          # softmax\n",
    "\n",
    "model = GCN(nfeat=features.shape[1], nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1649777846725,
     "user": {
      "displayName": "NH L",
      "userId": "13973463595760031265"
     },
     "user_tz": 240
    },
    "id": "0kahTMY9Z_Ml"
   },
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)                         \n",
    "    loss_train = torch.nn.functional.nll_loss(output[idx_train], labels[idx_train])  # loss function\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])                       # training accuracy\n",
    "    loss_train.backward()                                                            # backpropagation\n",
    "    optimizer.step()                                                                 # update parameters\n",
    "\n",
    "    if not fastmode:\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = torch.nn.functional.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8496,
     "status": "ok",
     "timestamp": 1649777855198,
     "user": {
      "displayName": "NH L",
      "userId": "13973463595760031265"
     },
     "user_tz": 240
    },
    "id": "FL0XvF23aCSI",
    "outputId": "766880e9-7845-4e49-d734-8f17499ef55e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.0425 acc_train: 0.1429 loss_val: 2.0737 acc_val: 0.1233 time: 0.0334s\n",
      "Epoch: 0002 loss_train: 2.0202 acc_train: 0.1500 loss_val: 2.0636 acc_val: 0.1200 time: 0.0133s\n",
      "Epoch: 0003 loss_train: 2.0012 acc_train: 0.1500 loss_val: 2.0414 acc_val: 0.1267 time: 0.0132s\n",
      "Epoch: 0004 loss_train: 1.9895 acc_train: 0.1571 loss_val: 2.0285 acc_val: 0.1300 time: 0.0150s\n",
      "Epoch: 0005 loss_train: 1.9719 acc_train: 0.2357 loss_val: 2.0072 acc_val: 0.1767 time: 0.0161s\n",
      "Epoch: 0006 loss_train: 1.9578 acc_train: 0.2500 loss_val: 1.9950 acc_val: 0.1633 time: 0.0144s\n",
      "Epoch: 0007 loss_train: 1.9560 acc_train: 0.2071 loss_val: 1.9835 acc_val: 0.1633 time: 0.0154s\n",
      "Epoch: 0008 loss_train: 1.9330 acc_train: 0.2000 loss_val: 1.9697 acc_val: 0.1567 time: 0.0160s\n",
      "Epoch: 0009 loss_train: 1.9115 acc_train: 0.2000 loss_val: 1.9526 acc_val: 0.1567 time: 0.0159s\n",
      "Epoch: 0010 loss_train: 1.9109 acc_train: 0.2000 loss_val: 1.9406 acc_val: 0.1567 time: 0.0182s\n",
      "Epoch: 0011 loss_train: 1.9011 acc_train: 0.2000 loss_val: 1.9143 acc_val: 0.1567 time: 0.0133s\n",
      "Epoch: 0012 loss_train: 1.8761 acc_train: 0.2000 loss_val: 1.9218 acc_val: 0.1567 time: 0.0131s\n",
      "Epoch: 0013 loss_train: 1.8753 acc_train: 0.2000 loss_val: 1.8985 acc_val: 0.1567 time: 0.0133s\n",
      "Epoch: 0014 loss_train: 1.8603 acc_train: 0.2000 loss_val: 1.8858 acc_val: 0.1567 time: 0.0169s\n",
      "Epoch: 0015 loss_train: 1.8477 acc_train: 0.2000 loss_val: 1.8710 acc_val: 0.1567 time: 0.0131s\n",
      "Epoch: 0016 loss_train: 1.8273 acc_train: 0.2000 loss_val: 1.8690 acc_val: 0.1600 time: 0.0142s\n",
      "Epoch: 0017 loss_train: 1.8294 acc_train: 0.2000 loss_val: 1.8492 acc_val: 0.1600 time: 0.0142s\n",
      "Epoch: 0018 loss_train: 1.7944 acc_train: 0.2357 loss_val: 1.8283 acc_val: 0.2133 time: 0.0134s\n",
      "Epoch: 0019 loss_train: 1.7950 acc_train: 0.2357 loss_val: 1.8112 acc_val: 0.2433 time: 0.0156s\n",
      "Epoch: 0020 loss_train: 1.7776 acc_train: 0.3500 loss_val: 1.8049 acc_val: 0.2700 time: 0.0160s\n",
      "Epoch: 0021 loss_train: 1.7753 acc_train: 0.3500 loss_val: 1.8155 acc_val: 0.3267 time: 0.0126s\n",
      "Epoch: 0022 loss_train: 1.7694 acc_train: 0.3143 loss_val: 1.7904 acc_val: 0.3200 time: 0.0135s\n",
      "Epoch: 0023 loss_train: 1.7701 acc_train: 0.3357 loss_val: 1.7967 acc_val: 0.3167 time: 0.0146s\n",
      "Epoch: 0024 loss_train: 1.7596 acc_train: 0.3429 loss_val: 1.7678 acc_val: 0.3433 time: 0.0171s\n",
      "Epoch: 0025 loss_train: 1.7518 acc_train: 0.3143 loss_val: 1.7591 acc_val: 0.3400 time: 0.0155s\n",
      "Epoch: 0026 loss_train: 1.7366 acc_train: 0.2929 loss_val: 1.7604 acc_val: 0.3667 time: 0.0150s\n",
      "Epoch: 0027 loss_train: 1.7216 acc_train: 0.3143 loss_val: 1.7397 acc_val: 0.3367 time: 0.0162s\n",
      "Epoch: 0028 loss_train: 1.7110 acc_train: 0.3000 loss_val: 1.7218 acc_val: 0.3533 time: 0.0152s\n",
      "Epoch: 0029 loss_train: 1.6987 acc_train: 0.3000 loss_val: 1.7362 acc_val: 0.3467 time: 0.0139s\n",
      "Epoch: 0030 loss_train: 1.7289 acc_train: 0.3214 loss_val: 1.7148 acc_val: 0.3467 time: 0.0148s\n",
      "Epoch: 0031 loss_train: 1.6999 acc_train: 0.3000 loss_val: 1.7210 acc_val: 0.3533 time: 0.0147s\n",
      "Epoch: 0032 loss_train: 1.6993 acc_train: 0.3000 loss_val: 1.7157 acc_val: 0.3567 time: 0.0152s\n",
      "Epoch: 0033 loss_train: 1.6888 acc_train: 0.3071 loss_val: 1.6867 acc_val: 0.3467 time: 0.0143s\n",
      "Epoch: 0034 loss_train: 1.7008 acc_train: 0.3000 loss_val: 1.7155 acc_val: 0.3500 time: 0.0186s\n",
      "Epoch: 0035 loss_train: 1.6412 acc_train: 0.3286 loss_val: 1.6845 acc_val: 0.3633 time: 0.0165s\n",
      "Epoch: 0036 loss_train: 1.6640 acc_train: 0.3214 loss_val: 1.6929 acc_val: 0.3500 time: 0.0175s\n",
      "Epoch: 0037 loss_train: 1.6546 acc_train: 0.3000 loss_val: 1.7050 acc_val: 0.3600 time: 0.0153s\n",
      "Epoch: 0038 loss_train: 1.6391 acc_train: 0.3286 loss_val: 1.7003 acc_val: 0.3533 time: 0.0145s\n",
      "Epoch: 0039 loss_train: 1.5960 acc_train: 0.3429 loss_val: 1.6650 acc_val: 0.3467 time: 0.0141s\n",
      "Epoch: 0040 loss_train: 1.6140 acc_train: 0.3643 loss_val: 1.6485 acc_val: 0.3733 time: 0.0185s\n",
      "Epoch: 0041 loss_train: 1.6015 acc_train: 0.3571 loss_val: 1.6499 acc_val: 0.3700 time: 0.0135s\n",
      "Epoch: 0042 loss_train: 1.5935 acc_train: 0.4000 loss_val: 1.6453 acc_val: 0.4000 time: 0.0159s\n",
      "Epoch: 0043 loss_train: 1.5303 acc_train: 0.4500 loss_val: 1.6460 acc_val: 0.4000 time: 0.0149s\n",
      "Epoch: 0044 loss_train: 1.5508 acc_train: 0.4286 loss_val: 1.6371 acc_val: 0.3733 time: 0.0166s\n",
      "Epoch: 0045 loss_train: 1.5437 acc_train: 0.4143 loss_val: 1.6292 acc_val: 0.4233 time: 0.0155s\n",
      "Epoch: 0046 loss_train: 1.5320 acc_train: 0.4429 loss_val: 1.5945 acc_val: 0.4300 time: 0.0149s\n",
      "Epoch: 0047 loss_train: 1.5006 acc_train: 0.4500 loss_val: 1.6174 acc_val: 0.3967 time: 0.0157s\n",
      "Epoch: 0048 loss_train: 1.4987 acc_train: 0.4786 loss_val: 1.5983 acc_val: 0.4167 time: 0.0146s\n",
      "Epoch: 0049 loss_train: 1.5014 acc_train: 0.4357 loss_val: 1.5750 acc_val: 0.4533 time: 0.0159s\n",
      "Epoch: 0050 loss_train: 1.4802 acc_train: 0.4643 loss_val: 1.5935 acc_val: 0.4300 time: 0.0146s\n",
      "Epoch: 0051 loss_train: 1.4417 acc_train: 0.4929 loss_val: 1.5394 acc_val: 0.4400 time: 0.0156s\n",
      "Epoch: 0052 loss_train: 1.4660 acc_train: 0.4714 loss_val: 1.5865 acc_val: 0.4200 time: 0.0148s\n",
      "Epoch: 0053 loss_train: 1.4292 acc_train: 0.5071 loss_val: 1.5351 acc_val: 0.5067 time: 0.0202s\n",
      "Epoch: 0054 loss_train: 1.3917 acc_train: 0.5429 loss_val: 1.4887 acc_val: 0.4833 time: 0.0156s\n",
      "Epoch: 0055 loss_train: 1.4342 acc_train: 0.5000 loss_val: 1.5232 acc_val: 0.4700 time: 0.0160s\n",
      "Epoch: 0056 loss_train: 1.3858 acc_train: 0.5214 loss_val: 1.4982 acc_val: 0.4567 time: 0.0202s\n",
      "Epoch: 0057 loss_train: 1.3671 acc_train: 0.5000 loss_val: 1.4951 acc_val: 0.4733 time: 0.0160s\n",
      "Epoch: 0058 loss_train: 1.3055 acc_train: 0.6143 loss_val: 1.4641 acc_val: 0.5167 time: 0.0178s\n",
      "Epoch: 0059 loss_train: 1.3256 acc_train: 0.6000 loss_val: 1.4946 acc_val: 0.5033 time: 0.0218s\n",
      "Epoch: 0060 loss_train: 1.2770 acc_train: 0.5786 loss_val: 1.4395 acc_val: 0.5200 time: 0.0181s\n",
      "Epoch: 0061 loss_train: 1.3324 acc_train: 0.6429 loss_val: 1.4593 acc_val: 0.5533 time: 0.0177s\n",
      "Epoch: 0062 loss_train: 1.2392 acc_train: 0.7000 loss_val: 1.3998 acc_val: 0.5967 time: 0.0152s\n",
      "Epoch: 0063 loss_train: 1.2482 acc_train: 0.6571 loss_val: 1.4099 acc_val: 0.5967 time: 0.0132s\n",
      "Epoch: 0064 loss_train: 1.2559 acc_train: 0.6714 loss_val: 1.4090 acc_val: 0.5533 time: 0.0157s\n",
      "Epoch: 0065 loss_train: 1.2114 acc_train: 0.7000 loss_val: 1.3838 acc_val: 0.5967 time: 0.0134s\n",
      "Epoch: 0066 loss_train: 1.2516 acc_train: 0.6929 loss_val: 1.4199 acc_val: 0.5533 time: 0.0130s\n",
      "Epoch: 0067 loss_train: 1.1761 acc_train: 0.6714 loss_val: 1.3998 acc_val: 0.5867 time: 0.0153s\n",
      "Epoch: 0068 loss_train: 1.1990 acc_train: 0.7429 loss_val: 1.3985 acc_val: 0.5700 time: 0.0155s\n",
      "Epoch: 0069 loss_train: 1.1399 acc_train: 0.7286 loss_val: 1.3451 acc_val: 0.6300 time: 0.0147s\n",
      "Epoch: 0070 loss_train: 1.1137 acc_train: 0.7214 loss_val: 1.3314 acc_val: 0.6433 time: 0.0157s\n",
      "Epoch: 0071 loss_train: 1.0882 acc_train: 0.7643 loss_val: 1.3070 acc_val: 0.6467 time: 0.0145s\n",
      "Epoch: 0072 loss_train: 1.1231 acc_train: 0.7429 loss_val: 1.3037 acc_val: 0.6200 time: 0.0140s\n",
      "Epoch: 0073 loss_train: 1.0984 acc_train: 0.7143 loss_val: 1.2905 acc_val: 0.6700 time: 0.0158s\n",
      "Epoch: 0074 loss_train: 1.1052 acc_train: 0.7143 loss_val: 1.2889 acc_val: 0.6467 time: 0.0157s\n",
      "Epoch: 0075 loss_train: 1.0446 acc_train: 0.7929 loss_val: 1.3030 acc_val: 0.6400 time: 0.0165s\n",
      "Epoch: 0076 loss_train: 1.0657 acc_train: 0.7357 loss_val: 1.2591 acc_val: 0.7067 time: 0.0163s\n",
      "Epoch: 0077 loss_train: 1.0331 acc_train: 0.7857 loss_val: 1.2319 acc_val: 0.6867 time: 0.0160s\n",
      "Epoch: 0078 loss_train: 1.0040 acc_train: 0.8357 loss_val: 1.2639 acc_val: 0.6767 time: 0.0131s\n",
      "Epoch: 0079 loss_train: 0.9892 acc_train: 0.8500 loss_val: 1.2257 acc_val: 0.7000 time: 0.0149s\n",
      "Epoch: 0080 loss_train: 0.9808 acc_train: 0.8071 loss_val: 1.2391 acc_val: 0.7133 time: 0.0147s\n",
      "Epoch: 0081 loss_train: 0.9540 acc_train: 0.8500 loss_val: 1.1822 acc_val: 0.7300 time: 0.0191s\n",
      "Epoch: 0082 loss_train: 0.9303 acc_train: 0.8214 loss_val: 1.1910 acc_val: 0.6800 time: 0.0147s\n",
      "Epoch: 0083 loss_train: 0.9767 acc_train: 0.8214 loss_val: 1.1873 acc_val: 0.6967 time: 0.0150s\n",
      "Epoch: 0084 loss_train: 0.9695 acc_train: 0.7929 loss_val: 1.2260 acc_val: 0.7200 time: 0.0158s\n",
      "Epoch: 0085 loss_train: 0.9054 acc_train: 0.8429 loss_val: 1.1629 acc_val: 0.7300 time: 0.0147s\n",
      "Epoch: 0086 loss_train: 0.9187 acc_train: 0.8214 loss_val: 1.1434 acc_val: 0.7267 time: 0.0138s\n",
      "Epoch: 0087 loss_train: 0.9435 acc_train: 0.8214 loss_val: 1.1740 acc_val: 0.6800 time: 0.0158s\n",
      "Epoch: 0088 loss_train: 0.8574 acc_train: 0.8500 loss_val: 1.1021 acc_val: 0.7133 time: 0.0194s\n",
      "Epoch: 0089 loss_train: 0.8614 acc_train: 0.8071 loss_val: 1.0983 acc_val: 0.7233 time: 0.0155s\n",
      "Epoch: 0090 loss_train: 0.8859 acc_train: 0.8214 loss_val: 1.1158 acc_val: 0.7367 time: 0.0183s\n",
      "Epoch: 0091 loss_train: 0.8915 acc_train: 0.8286 loss_val: 1.1046 acc_val: 0.7267 time: 0.0173s\n",
      "Epoch: 0092 loss_train: 0.8548 acc_train: 0.8214 loss_val: 1.1080 acc_val: 0.7100 time: 0.0160s\n",
      "Epoch: 0093 loss_train: 0.8559 acc_train: 0.8357 loss_val: 1.0790 acc_val: 0.7533 time: 0.0169s\n",
      "Epoch: 0094 loss_train: 0.8229 acc_train: 0.8429 loss_val: 1.0864 acc_val: 0.7233 time: 0.0168s\n",
      "Epoch: 0095 loss_train: 0.8159 acc_train: 0.8500 loss_val: 1.0968 acc_val: 0.7333 time: 0.0133s\n",
      "Epoch: 0096 loss_train: 0.8236 acc_train: 0.8429 loss_val: 1.0986 acc_val: 0.7167 time: 0.0133s\n",
      "Epoch: 0097 loss_train: 0.8413 acc_train: 0.8429 loss_val: 1.1231 acc_val: 0.6933 time: 0.0141s\n",
      "Epoch: 0098 loss_train: 0.7515 acc_train: 0.8571 loss_val: 1.0174 acc_val: 0.7433 time: 0.0141s\n",
      "Epoch: 0099 loss_train: 0.8042 acc_train: 0.8214 loss_val: 1.0366 acc_val: 0.7733 time: 0.0162s\n",
      "Epoch: 0100 loss_train: 0.7998 acc_train: 0.8786 loss_val: 1.1153 acc_val: 0.7200 time: 0.0169s\n",
      "Epoch: 0101 loss_train: 0.7688 acc_train: 0.8714 loss_val: 1.0315 acc_val: 0.7467 time: 0.0140s\n",
      "Epoch: 0102 loss_train: 0.7548 acc_train: 0.8357 loss_val: 1.0339 acc_val: 0.7333 time: 0.0168s\n",
      "Epoch: 0103 loss_train: 0.7343 acc_train: 0.8857 loss_val: 1.0709 acc_val: 0.7367 time: 0.0164s\n",
      "Epoch: 0104 loss_train: 0.7248 acc_train: 0.8500 loss_val: 1.0207 acc_val: 0.7400 time: 0.0133s\n",
      "Epoch: 0105 loss_train: 0.7473 acc_train: 0.8643 loss_val: 1.0024 acc_val: 0.7533 time: 0.0184s\n",
      "Epoch: 0106 loss_train: 0.7396 acc_train: 0.8643 loss_val: 1.0355 acc_val: 0.7433 time: 0.0188s\n",
      "Epoch: 0107 loss_train: 0.7231 acc_train: 0.8714 loss_val: 1.0874 acc_val: 0.7133 time: 0.0162s\n",
      "Epoch: 0108 loss_train: 0.7161 acc_train: 0.8786 loss_val: 1.0151 acc_val: 0.7333 time: 0.0164s\n",
      "Epoch: 0109 loss_train: 0.6882 acc_train: 0.8786 loss_val: 0.9865 acc_val: 0.7633 time: 0.0170s\n",
      "Epoch: 0110 loss_train: 0.7122 acc_train: 0.8714 loss_val: 0.9891 acc_val: 0.7333 time: 0.0164s\n",
      "Epoch: 0111 loss_train: 0.6684 acc_train: 0.8786 loss_val: 0.9446 acc_val: 0.7500 time: 0.0162s\n",
      "Epoch: 0112 loss_train: 0.6567 acc_train: 0.8786 loss_val: 0.9602 acc_val: 0.7467 time: 0.0161s\n",
      "Epoch: 0113 loss_train: 0.7140 acc_train: 0.8286 loss_val: 0.9685 acc_val: 0.7333 time: 0.0162s\n",
      "Epoch: 0114 loss_train: 0.6849 acc_train: 0.8500 loss_val: 0.9709 acc_val: 0.7367 time: 0.0152s\n",
      "Epoch: 0115 loss_train: 0.6797 acc_train: 0.8571 loss_val: 0.9897 acc_val: 0.7433 time: 0.0202s\n",
      "Epoch: 0116 loss_train: 0.6801 acc_train: 0.8429 loss_val: 0.9436 acc_val: 0.7467 time: 0.0139s\n",
      "Epoch: 0117 loss_train: 0.6694 acc_train: 0.8786 loss_val: 0.9614 acc_val: 0.7167 time: 0.0156s\n",
      "Epoch: 0118 loss_train: 0.6469 acc_train: 0.8571 loss_val: 0.9369 acc_val: 0.7167 time: 0.0187s\n",
      "Epoch: 0119 loss_train: 0.6589 acc_train: 0.8714 loss_val: 0.9903 acc_val: 0.7167 time: 0.0169s\n",
      "Epoch: 0120 loss_train: 0.6168 acc_train: 0.8929 loss_val: 0.9408 acc_val: 0.7800 time: 0.0149s\n",
      "Epoch: 0121 loss_train: 0.6345 acc_train: 0.8857 loss_val: 0.9232 acc_val: 0.7833 time: 0.0228s\n",
      "Epoch: 0122 loss_train: 0.6457 acc_train: 0.9071 loss_val: 0.9663 acc_val: 0.7300 time: 0.0141s\n",
      "Epoch: 0123 loss_train: 0.5827 acc_train: 0.9071 loss_val: 0.9365 acc_val: 0.7467 time: 0.0175s\n",
      "Epoch: 0124 loss_train: 0.6358 acc_train: 0.8857 loss_val: 0.9313 acc_val: 0.7633 time: 0.0169s\n",
      "Epoch: 0125 loss_train: 0.5863 acc_train: 0.8714 loss_val: 0.8816 acc_val: 0.7700 time: 0.0156s\n",
      "Epoch: 0126 loss_train: 0.6358 acc_train: 0.8643 loss_val: 0.9169 acc_val: 0.7733 time: 0.0164s\n",
      "Epoch: 0127 loss_train: 0.6366 acc_train: 0.8929 loss_val: 0.8901 acc_val: 0.7800 time: 0.0180s\n",
      "Epoch: 0128 loss_train: 0.5755 acc_train: 0.9071 loss_val: 0.9168 acc_val: 0.7500 time: 0.0169s\n",
      "Epoch: 0129 loss_train: 0.6264 acc_train: 0.8857 loss_val: 0.8954 acc_val: 0.7667 time: 0.0197s\n",
      "Epoch: 0130 loss_train: 0.5975 acc_train: 0.8929 loss_val: 0.9048 acc_val: 0.7533 time: 0.0177s\n",
      "Epoch: 0131 loss_train: 0.5555 acc_train: 0.9071 loss_val: 0.9074 acc_val: 0.7767 time: 0.0194s\n",
      "Epoch: 0132 loss_train: 0.5823 acc_train: 0.8857 loss_val: 0.9035 acc_val: 0.7400 time: 0.0165s\n",
      "Epoch: 0133 loss_train: 0.5912 acc_train: 0.8786 loss_val: 0.9200 acc_val: 0.7567 time: 0.0161s\n",
      "Epoch: 0134 loss_train: 0.5551 acc_train: 0.8929 loss_val: 0.8905 acc_val: 0.7633 time: 0.0171s\n",
      "Epoch: 0135 loss_train: 0.5598 acc_train: 0.9214 loss_val: 0.8582 acc_val: 0.7667 time: 0.0172s\n",
      "Epoch: 0136 loss_train: 0.5686 acc_train: 0.9214 loss_val: 0.8656 acc_val: 0.7767 time: 0.0176s\n",
      "Epoch: 0137 loss_train: 0.5623 acc_train: 0.9000 loss_val: 0.8520 acc_val: 0.7600 time: 0.0157s\n",
      "Epoch: 0138 loss_train: 0.5637 acc_train: 0.9143 loss_val: 0.9027 acc_val: 0.7833 time: 0.0193s\n",
      "Epoch: 0139 loss_train: 0.5268 acc_train: 0.9071 loss_val: 0.9120 acc_val: 0.7433 time: 0.0174s\n",
      "Epoch: 0140 loss_train: 0.5470 acc_train: 0.8786 loss_val: 0.9065 acc_val: 0.7467 time: 0.0167s\n",
      "Epoch: 0141 loss_train: 0.4692 acc_train: 0.9429 loss_val: 0.8705 acc_val: 0.7700 time: 0.0139s\n",
      "Epoch: 0142 loss_train: 0.4955 acc_train: 0.9143 loss_val: 0.8185 acc_val: 0.7900 time: 0.0169s\n",
      "Epoch: 0143 loss_train: 0.5267 acc_train: 0.9000 loss_val: 0.8732 acc_val: 0.7700 time: 0.0134s\n",
      "Epoch: 0144 loss_train: 0.5240 acc_train: 0.9143 loss_val: 0.8572 acc_val: 0.7900 time: 0.0159s\n",
      "Epoch: 0145 loss_train: 0.5078 acc_train: 0.8929 loss_val: 0.8513 acc_val: 0.7633 time: 0.0156s\n",
      "Epoch: 0146 loss_train: 0.5048 acc_train: 0.9214 loss_val: 0.8846 acc_val: 0.7500 time: 0.0161s\n",
      "Epoch: 0147 loss_train: 0.5110 acc_train: 0.9357 loss_val: 0.8504 acc_val: 0.7833 time: 0.0158s\n",
      "Epoch: 0148 loss_train: 0.5165 acc_train: 0.9000 loss_val: 0.8550 acc_val: 0.7733 time: 0.0163s\n",
      "Epoch: 0149 loss_train: 0.5136 acc_train: 0.9071 loss_val: 0.8909 acc_val: 0.7633 time: 0.0159s\n",
      "Epoch: 0150 loss_train: 0.5119 acc_train: 0.9143 loss_val: 0.8831 acc_val: 0.7467 time: 0.0195s\n",
      "Epoch: 0151 loss_train: 0.5019 acc_train: 0.9000 loss_val: 0.8533 acc_val: 0.7833 time: 0.0143s\n",
      "Epoch: 0152 loss_train: 0.4782 acc_train: 0.9357 loss_val: 0.8336 acc_val: 0.7900 time: 0.0155s\n",
      "Epoch: 0153 loss_train: 0.5479 acc_train: 0.8429 loss_val: 0.8435 acc_val: 0.7733 time: 0.0169s\n",
      "Epoch: 0154 loss_train: 0.4665 acc_train: 0.9214 loss_val: 0.8404 acc_val: 0.7533 time: 0.0153s\n",
      "Epoch: 0155 loss_train: 0.4429 acc_train: 0.9143 loss_val: 0.8652 acc_val: 0.7600 time: 0.0154s\n",
      "Epoch: 0156 loss_train: 0.5013 acc_train: 0.9286 loss_val: 0.8777 acc_val: 0.7633 time: 0.0186s\n",
      "Epoch: 0157 loss_train: 0.4765 acc_train: 0.9214 loss_val: 0.8522 acc_val: 0.7800 time: 0.0172s\n",
      "Epoch: 0158 loss_train: 0.4482 acc_train: 0.9500 loss_val: 0.7922 acc_val: 0.7833 time: 0.0155s\n",
      "Epoch: 0159 loss_train: 0.4937 acc_train: 0.9071 loss_val: 0.8709 acc_val: 0.7400 time: 0.0202s\n",
      "Epoch: 0160 loss_train: 0.4939 acc_train: 0.8929 loss_val: 0.8637 acc_val: 0.7567 time: 0.0148s\n",
      "Epoch: 0161 loss_train: 0.5025 acc_train: 0.9000 loss_val: 0.8490 acc_val: 0.7633 time: 0.0174s\n",
      "Epoch: 0162 loss_train: 0.4609 acc_train: 0.9500 loss_val: 0.8878 acc_val: 0.7567 time: 0.0192s\n",
      "Epoch: 0163 loss_train: 0.4709 acc_train: 0.9214 loss_val: 0.8122 acc_val: 0.7700 time: 0.0175s\n",
      "Epoch: 0164 loss_train: 0.4660 acc_train: 0.9357 loss_val: 0.8028 acc_val: 0.7867 time: 0.0157s\n",
      "Epoch: 0165 loss_train: 0.4801 acc_train: 0.9357 loss_val: 0.8121 acc_val: 0.7633 time: 0.0153s\n",
      "Epoch: 0166 loss_train: 0.4354 acc_train: 0.9500 loss_val: 0.8139 acc_val: 0.7933 time: 0.0168s\n",
      "Epoch: 0167 loss_train: 0.4684 acc_train: 0.9429 loss_val: 0.8565 acc_val: 0.7367 time: 0.0159s\n",
      "Epoch: 0168 loss_train: 0.4981 acc_train: 0.9143 loss_val: 0.8458 acc_val: 0.7667 time: 0.0161s\n",
      "Epoch: 0169 loss_train: 0.4653 acc_train: 0.9071 loss_val: 0.8143 acc_val: 0.7867 time: 0.0158s\n",
      "Epoch: 0170 loss_train: 0.4423 acc_train: 0.9214 loss_val: 0.8507 acc_val: 0.7867 time: 0.0159s\n",
      "Epoch: 0171 loss_train: 0.4432 acc_train: 0.9071 loss_val: 0.7993 acc_val: 0.7567 time: 0.0149s\n",
      "Epoch: 0172 loss_train: 0.4511 acc_train: 0.9000 loss_val: 0.8087 acc_val: 0.7700 time: 0.0150s\n",
      "Epoch: 0173 loss_train: 0.4515 acc_train: 0.9429 loss_val: 0.8025 acc_val: 0.7633 time: 0.0180s\n",
      "Epoch: 0174 loss_train: 0.4619 acc_train: 0.9357 loss_val: 0.8560 acc_val: 0.7667 time: 0.0189s\n",
      "Epoch: 0175 loss_train: 0.4529 acc_train: 0.9143 loss_val: 0.7945 acc_val: 0.7667 time: 0.0133s\n",
      "Epoch: 0176 loss_train: 0.4545 acc_train: 0.9429 loss_val: 0.8318 acc_val: 0.7833 time: 0.0132s\n",
      "Epoch: 0177 loss_train: 0.4284 acc_train: 0.9357 loss_val: 0.8280 acc_val: 0.7700 time: 0.0133s\n",
      "Epoch: 0178 loss_train: 0.4834 acc_train: 0.9214 loss_val: 0.8253 acc_val: 0.7567 time: 0.0168s\n",
      "Epoch: 0179 loss_train: 0.4222 acc_train: 0.9429 loss_val: 0.7543 acc_val: 0.7933 time: 0.0198s\n",
      "Epoch: 0180 loss_train: 0.4329 acc_train: 0.9286 loss_val: 0.7912 acc_val: 0.7800 time: 0.0171s\n",
      "Epoch: 0181 loss_train: 0.3991 acc_train: 0.9286 loss_val: 0.8026 acc_val: 0.7600 time: 0.0156s\n",
      "Epoch: 0182 loss_train: 0.4078 acc_train: 0.9500 loss_val: 0.8460 acc_val: 0.7367 time: 0.0158s\n",
      "Epoch: 0183 loss_train: 0.4628 acc_train: 0.9000 loss_val: 0.8590 acc_val: 0.7700 time: 0.0147s\n",
      "Epoch: 0184 loss_train: 0.4332 acc_train: 0.9500 loss_val: 0.7838 acc_val: 0.7600 time: 0.0190s\n",
      "Epoch: 0185 loss_train: 0.4471 acc_train: 0.9357 loss_val: 0.9174 acc_val: 0.7400 time: 0.0161s\n",
      "Epoch: 0186 loss_train: 0.4051 acc_train: 0.9357 loss_val: 0.8340 acc_val: 0.7500 time: 0.0160s\n",
      "Epoch: 0187 loss_train: 0.4431 acc_train: 0.9357 loss_val: 0.7566 acc_val: 0.7900 time: 0.0171s\n",
      "Epoch: 0188 loss_train: 0.4509 acc_train: 0.9286 loss_val: 0.8050 acc_val: 0.7533 time: 0.0162s\n",
      "Epoch: 0189 loss_train: 0.4456 acc_train: 0.9286 loss_val: 0.7854 acc_val: 0.7567 time: 0.0159s\n",
      "Epoch: 0190 loss_train: 0.4252 acc_train: 0.9500 loss_val: 0.7724 acc_val: 0.8133 time: 0.0160s\n",
      "Epoch: 0191 loss_train: 0.3951 acc_train: 0.9429 loss_val: 0.7566 acc_val: 0.8000 time: 0.0169s\n",
      "Epoch: 0192 loss_train: 0.4444 acc_train: 0.9357 loss_val: 0.7556 acc_val: 0.7800 time: 0.0143s\n",
      "Epoch: 0193 loss_train: 0.4426 acc_train: 0.9143 loss_val: 0.8122 acc_val: 0.7933 time: 0.0146s\n",
      "Epoch: 0194 loss_train: 0.4025 acc_train: 0.9071 loss_val: 0.8056 acc_val: 0.7500 time: 0.0136s\n",
      "Epoch: 0195 loss_train: 0.4120 acc_train: 0.9571 loss_val: 0.8339 acc_val: 0.7500 time: 0.0161s\n",
      "Epoch: 0196 loss_train: 0.4154 acc_train: 0.9214 loss_val: 0.8294 acc_val: 0.7733 time: 0.0161s\n",
      "Epoch: 0197 loss_train: 0.3939 acc_train: 0.9214 loss_val: 0.8088 acc_val: 0.7833 time: 0.0151s\n",
      "Epoch: 0198 loss_train: 0.4000 acc_train: 0.9286 loss_val: 0.8093 acc_val: 0.7633 time: 0.0178s\n",
      "Epoch: 0199 loss_train: 0.4069 acc_train: 0.9500 loss_val: 0.8205 acc_val: 0.7700 time: 0.0182s\n",
      "Epoch: 0200 loss_train: 0.4151 acc_train: 0.9286 loss_val: 0.8167 acc_val: 0.7600 time: 0.0162s\n",
      "Epoch: 0201 loss_train: 0.4031 acc_train: 0.9429 loss_val: 0.7449 acc_val: 0.7767 time: 0.0160s\n",
      "Epoch: 0202 loss_train: 0.4583 acc_train: 0.9357 loss_val: 0.8315 acc_val: 0.7733 time: 0.0173s\n",
      "Epoch: 0203 loss_train: 0.3861 acc_train: 0.9571 loss_val: 0.7430 acc_val: 0.7800 time: 0.0154s\n",
      "Epoch: 0204 loss_train: 0.4143 acc_train: 0.9214 loss_val: 0.7748 acc_val: 0.7767 time: 0.0146s\n",
      "Epoch: 0205 loss_train: 0.3574 acc_train: 0.9714 loss_val: 0.7762 acc_val: 0.7700 time: 0.0160s\n",
      "Epoch: 0206 loss_train: 0.4036 acc_train: 0.9429 loss_val: 0.8227 acc_val: 0.7900 time: 0.0146s\n",
      "Epoch: 0207 loss_train: 0.4020 acc_train: 0.9357 loss_val: 0.7681 acc_val: 0.7733 time: 0.0158s\n",
      "Epoch: 0208 loss_train: 0.3688 acc_train: 0.9571 loss_val: 0.8002 acc_val: 0.7867 time: 0.0150s\n",
      "Epoch: 0209 loss_train: 0.4193 acc_train: 0.9500 loss_val: 0.7979 acc_val: 0.7700 time: 0.0153s\n",
      "Epoch: 0210 loss_train: 0.3831 acc_train: 0.9500 loss_val: 0.7926 acc_val: 0.7733 time: 0.0143s\n",
      "Epoch: 0211 loss_train: 0.3984 acc_train: 0.9357 loss_val: 0.7860 acc_val: 0.7867 time: 0.0161s\n",
      "Epoch: 0212 loss_train: 0.3968 acc_train: 0.9357 loss_val: 0.7981 acc_val: 0.7633 time: 0.0187s\n",
      "Epoch: 0213 loss_train: 0.3767 acc_train: 0.9286 loss_val: 0.7387 acc_val: 0.8133 time: 0.0151s\n",
      "Epoch: 0214 loss_train: 0.3570 acc_train: 0.9357 loss_val: 0.8180 acc_val: 0.7567 time: 0.0148s\n",
      "Epoch: 0215 loss_train: 0.3804 acc_train: 0.9500 loss_val: 0.7646 acc_val: 0.7533 time: 0.0137s\n",
      "Epoch: 0216 loss_train: 0.4038 acc_train: 0.9500 loss_val: 0.7808 acc_val: 0.7933 time: 0.0152s\n",
      "Epoch: 0217 loss_train: 0.3569 acc_train: 0.9714 loss_val: 0.7458 acc_val: 0.7900 time: 0.0159s\n",
      "Epoch: 0218 loss_train: 0.3523 acc_train: 0.9571 loss_val: 0.7349 acc_val: 0.7767 time: 0.0163s\n",
      "Epoch: 0219 loss_train: 0.3851 acc_train: 0.9357 loss_val: 0.8143 acc_val: 0.7667 time: 0.0146s\n",
      "Epoch: 0220 loss_train: 0.3585 acc_train: 0.9286 loss_val: 0.7817 acc_val: 0.7967 time: 0.0165s\n",
      "Epoch: 0221 loss_train: 0.3846 acc_train: 0.9357 loss_val: 0.8015 acc_val: 0.7767 time: 0.0157s\n",
      "Epoch: 0222 loss_train: 0.3778 acc_train: 0.9500 loss_val: 0.7910 acc_val: 0.7833 time: 0.0162s\n",
      "Epoch: 0223 loss_train: 0.3358 acc_train: 0.9500 loss_val: 0.7658 acc_val: 0.7700 time: 0.0167s\n",
      "Epoch: 0224 loss_train: 0.3716 acc_train: 0.9214 loss_val: 0.7939 acc_val: 0.7633 time: 0.0167s\n",
      "Epoch: 0225 loss_train: 0.3621 acc_train: 0.9571 loss_val: 0.7863 acc_val: 0.7833 time: 0.0197s\n",
      "Epoch: 0226 loss_train: 0.3676 acc_train: 0.9357 loss_val: 0.8142 acc_val: 0.7700 time: 0.0159s\n",
      "Epoch: 0227 loss_train: 0.4041 acc_train: 0.9357 loss_val: 0.7445 acc_val: 0.7800 time: 0.0141s\n",
      "Epoch: 0228 loss_train: 0.3583 acc_train: 0.9500 loss_val: 0.7362 acc_val: 0.7800 time: 0.0163s\n",
      "Epoch: 0229 loss_train: 0.3499 acc_train: 0.9571 loss_val: 0.7601 acc_val: 0.7800 time: 0.0162s\n",
      "Epoch: 0230 loss_train: 0.3745 acc_train: 0.9500 loss_val: 0.7645 acc_val: 0.7800 time: 0.0162s\n",
      "Epoch: 0231 loss_train: 0.3666 acc_train: 0.9429 loss_val: 0.7645 acc_val: 0.7933 time: 0.0171s\n",
      "Epoch: 0232 loss_train: 0.3960 acc_train: 0.9357 loss_val: 0.7514 acc_val: 0.8167 time: 0.0178s\n",
      "Epoch: 0233 loss_train: 0.3535 acc_train: 0.9786 loss_val: 0.7521 acc_val: 0.7900 time: 0.0193s\n",
      "Epoch: 0234 loss_train: 0.3940 acc_train: 0.9500 loss_val: 0.7991 acc_val: 0.7833 time: 0.0158s\n",
      "Epoch: 0235 loss_train: 0.3807 acc_train: 0.9357 loss_val: 0.8379 acc_val: 0.7800 time: 0.0154s\n",
      "Epoch: 0236 loss_train: 0.3948 acc_train: 0.9429 loss_val: 0.7861 acc_val: 0.7667 time: 0.0163s\n",
      "Epoch: 0237 loss_train: 0.3749 acc_train: 0.9429 loss_val: 0.7703 acc_val: 0.7767 time: 0.0158s\n",
      "Epoch: 0238 loss_train: 0.3388 acc_train: 0.9643 loss_val: 0.7700 acc_val: 0.7767 time: 0.0147s\n",
      "Epoch: 0239 loss_train: 0.3791 acc_train: 0.9429 loss_val: 0.7729 acc_val: 0.7567 time: 0.0205s\n",
      "Epoch: 0240 loss_train: 0.3613 acc_train: 0.9214 loss_val: 0.8158 acc_val: 0.7567 time: 0.0134s\n",
      "Epoch: 0241 loss_train: 0.3659 acc_train: 0.9357 loss_val: 0.7868 acc_val: 0.7633 time: 0.0130s\n",
      "Epoch: 0242 loss_train: 0.3569 acc_train: 0.9429 loss_val: 0.7884 acc_val: 0.7767 time: 0.0132s\n",
      "Epoch: 0243 loss_train: 0.3423 acc_train: 0.9714 loss_val: 0.7689 acc_val: 0.7467 time: 0.0151s\n",
      "Epoch: 0244 loss_train: 0.3496 acc_train: 0.9714 loss_val: 0.7868 acc_val: 0.7900 time: 0.0144s\n",
      "Epoch: 0245 loss_train: 0.3498 acc_train: 0.9571 loss_val: 0.7597 acc_val: 0.7900 time: 0.0148s\n",
      "Epoch: 0246 loss_train: 0.3597 acc_train: 0.9500 loss_val: 0.8121 acc_val: 0.7433 time: 0.0148s\n",
      "Epoch: 0247 loss_train: 0.4130 acc_train: 0.9214 loss_val: 0.7893 acc_val: 0.7600 time: 0.0164s\n",
      "Epoch: 0248 loss_train: 0.3625 acc_train: 0.9571 loss_val: 0.7967 acc_val: 0.7567 time: 0.0161s\n",
      "Epoch: 0249 loss_train: 0.3419 acc_train: 0.9571 loss_val: 0.7436 acc_val: 0.7800 time: 0.0219s\n",
      "Epoch: 0250 loss_train: 0.3335 acc_train: 0.9357 loss_val: 0.7369 acc_val: 0.8000 time: 0.0133s\n",
      "Epoch: 0251 loss_train: 0.3564 acc_train: 0.9714 loss_val: 0.7458 acc_val: 0.7833 time: 0.0135s\n",
      "Epoch: 0252 loss_train: 0.3374 acc_train: 0.9429 loss_val: 0.7683 acc_val: 0.7900 time: 0.0150s\n",
      "Epoch: 0253 loss_train: 0.3465 acc_train: 0.9357 loss_val: 0.7039 acc_val: 0.7967 time: 0.0155s\n",
      "Epoch: 0254 loss_train: 0.3565 acc_train: 0.9429 loss_val: 0.7445 acc_val: 0.8167 time: 0.0140s\n",
      "Epoch: 0255 loss_train: 0.3617 acc_train: 0.9643 loss_val: 0.7609 acc_val: 0.8033 time: 0.0164s\n",
      "Epoch: 0256 loss_train: 0.3324 acc_train: 0.9429 loss_val: 0.7489 acc_val: 0.7967 time: 0.0160s\n",
      "Epoch: 0257 loss_train: 0.3596 acc_train: 0.9357 loss_val: 0.7658 acc_val: 0.7833 time: 0.0159s\n",
      "Epoch: 0258 loss_train: 0.3190 acc_train: 0.9786 loss_val: 0.7393 acc_val: 0.7933 time: 0.0150s\n",
      "Epoch: 0259 loss_train: 0.3440 acc_train: 0.9643 loss_val: 0.7726 acc_val: 0.7833 time: 0.0163s\n",
      "Epoch: 0260 loss_train: 0.3662 acc_train: 0.9714 loss_val: 0.7505 acc_val: 0.7700 time: 0.0140s\n",
      "Epoch: 0261 loss_train: 0.3139 acc_train: 0.9643 loss_val: 0.7079 acc_val: 0.8033 time: 0.0174s\n",
      "Epoch: 0262 loss_train: 0.3168 acc_train: 0.9643 loss_val: 0.7304 acc_val: 0.7633 time: 0.0202s\n",
      "Epoch: 0263 loss_train: 0.3372 acc_train: 0.9643 loss_val: 0.7940 acc_val: 0.7633 time: 0.0135s\n",
      "Epoch: 0264 loss_train: 0.3543 acc_train: 0.9571 loss_val: 0.7788 acc_val: 0.7867 time: 0.0139s\n",
      "Epoch: 0265 loss_train: 0.3331 acc_train: 0.9714 loss_val: 0.7491 acc_val: 0.8000 time: 0.0136s\n",
      "Epoch: 0266 loss_train: 0.3135 acc_train: 0.9714 loss_val: 0.7546 acc_val: 0.7900 time: 0.0155s\n",
      "Epoch: 0267 loss_train: 0.3584 acc_train: 0.9286 loss_val: 0.8176 acc_val: 0.7633 time: 0.0148s\n",
      "Epoch: 0268 loss_train: 0.3527 acc_train: 0.9500 loss_val: 0.7728 acc_val: 0.7700 time: 0.0137s\n",
      "Epoch: 0269 loss_train: 0.3315 acc_train: 0.9500 loss_val: 0.7592 acc_val: 0.7767 time: 0.0166s\n",
      "Epoch: 0270 loss_train: 0.3175 acc_train: 0.9643 loss_val: 0.8068 acc_val: 0.7600 time: 0.0156s\n",
      "Epoch: 0271 loss_train: 0.3472 acc_train: 0.9500 loss_val: 0.7379 acc_val: 0.7867 time: 0.0149s\n",
      "Epoch: 0272 loss_train: 0.3349 acc_train: 0.9714 loss_val: 0.7800 acc_val: 0.7833 time: 0.0172s\n",
      "Epoch: 0273 loss_train: 0.2943 acc_train: 0.9571 loss_val: 0.8374 acc_val: 0.7533 time: 0.0171s\n",
      "Epoch: 0274 loss_train: 0.3210 acc_train: 0.9714 loss_val: 0.7477 acc_val: 0.8033 time: 0.0167s\n",
      "Epoch: 0275 loss_train: 0.3599 acc_train: 0.9429 loss_val: 0.7677 acc_val: 0.7667 time: 0.0194s\n",
      "Epoch: 0276 loss_train: 0.3044 acc_train: 0.9500 loss_val: 0.7922 acc_val: 0.7933 time: 0.0162s\n",
      "Epoch: 0277 loss_train: 0.3237 acc_train: 0.9571 loss_val: 0.7524 acc_val: 0.7867 time: 0.0162s\n",
      "Epoch: 0278 loss_train: 0.2934 acc_train: 0.9786 loss_val: 0.7530 acc_val: 0.7733 time: 0.0168s\n",
      "Epoch: 0279 loss_train: 0.3325 acc_train: 0.9571 loss_val: 0.7250 acc_val: 0.7733 time: 0.0160s\n",
      "Epoch: 0280 loss_train: 0.2841 acc_train: 0.9786 loss_val: 0.7417 acc_val: 0.7900 time: 0.0190s\n",
      "Epoch: 0281 loss_train: 0.3525 acc_train: 0.9500 loss_val: 0.7947 acc_val: 0.7900 time: 0.0166s\n",
      "Epoch: 0282 loss_train: 0.3212 acc_train: 0.9500 loss_val: 0.7213 acc_val: 0.8167 time: 0.0157s\n",
      "Epoch: 0283 loss_train: 0.2954 acc_train: 0.9857 loss_val: 0.7433 acc_val: 0.7833 time: 0.0162s\n",
      "Epoch: 0284 loss_train: 0.2972 acc_train: 0.9643 loss_val: 0.7991 acc_val: 0.7800 time: 0.0164s\n",
      "Epoch: 0285 loss_train: 0.2974 acc_train: 0.9714 loss_val: 0.7744 acc_val: 0.7633 time: 0.0146s\n",
      "Epoch: 0286 loss_train: 0.3021 acc_train: 0.9786 loss_val: 0.7691 acc_val: 0.7933 time: 0.0177s\n",
      "Epoch: 0287 loss_train: 0.2989 acc_train: 0.9643 loss_val: 0.7427 acc_val: 0.7867 time: 0.0151s\n",
      "Epoch: 0288 loss_train: 0.3027 acc_train: 0.9643 loss_val: 0.7909 acc_val: 0.7600 time: 0.0149s\n",
      "Epoch: 0289 loss_train: 0.2971 acc_train: 0.9571 loss_val: 0.7604 acc_val: 0.7833 time: 0.0149s\n",
      "Epoch: 0290 loss_train: 0.3304 acc_train: 0.9357 loss_val: 0.7789 acc_val: 0.7667 time: 0.0159s\n",
      "Epoch: 0291 loss_train: 0.2750 acc_train: 0.9643 loss_val: 0.7094 acc_val: 0.8100 time: 0.0178s\n",
      "Epoch: 0292 loss_train: 0.3263 acc_train: 0.9714 loss_val: 0.7567 acc_val: 0.7667 time: 0.0141s\n",
      "Epoch: 0293 loss_train: 0.3242 acc_train: 0.9571 loss_val: 0.7857 acc_val: 0.7633 time: 0.0159s\n",
      "Epoch: 0294 loss_train: 0.3034 acc_train: 0.9857 loss_val: 0.7864 acc_val: 0.7667 time: 0.0158s\n",
      "Epoch: 0295 loss_train: 0.3152 acc_train: 0.9714 loss_val: 0.7032 acc_val: 0.7800 time: 0.0159s\n",
      "Epoch: 0296 loss_train: 0.2858 acc_train: 0.9643 loss_val: 0.7412 acc_val: 0.7933 time: 0.0146s\n",
      "Epoch: 0297 loss_train: 0.3363 acc_train: 0.9357 loss_val: 0.7039 acc_val: 0.8067 time: 0.0143s\n",
      "Epoch: 0298 loss_train: 0.3119 acc_train: 0.9500 loss_val: 0.7209 acc_val: 0.7900 time: 0.0161s\n",
      "Epoch: 0299 loss_train: 0.2914 acc_train: 0.9643 loss_val: 0.7440 acc_val: 0.7633 time: 0.0222s\n",
      "Epoch: 0300 loss_train: 0.3287 acc_train: 0.9429 loss_val: 0.7924 acc_val: 0.7633 time: 0.0136s\n",
      "Epoch: 0301 loss_train: 0.3460 acc_train: 0.9500 loss_val: 0.7491 acc_val: 0.7867 time: 0.0135s\n",
      "Epoch: 0302 loss_train: 0.2939 acc_train: 0.9643 loss_val: 0.7200 acc_val: 0.7833 time: 0.0152s\n",
      "Epoch: 0303 loss_train: 0.2779 acc_train: 0.9714 loss_val: 0.7167 acc_val: 0.7667 time: 0.0150s\n",
      "Epoch: 0304 loss_train: 0.3084 acc_train: 0.9643 loss_val: 0.7197 acc_val: 0.7967 time: 0.0155s\n",
      "Epoch: 0305 loss_train: 0.3292 acc_train: 0.9714 loss_val: 0.7386 acc_val: 0.7967 time: 0.0164s\n",
      "Epoch: 0306 loss_train: 0.2823 acc_train: 0.9714 loss_val: 0.6956 acc_val: 0.8200 time: 0.0155s\n",
      "Epoch: 0307 loss_train: 0.2978 acc_train: 0.9500 loss_val: 0.7418 acc_val: 0.7633 time: 0.0172s\n",
      "Epoch: 0308 loss_train: 0.2892 acc_train: 0.9786 loss_val: 0.7795 acc_val: 0.7767 time: 0.0170s\n",
      "Epoch: 0309 loss_train: 0.3104 acc_train: 0.9500 loss_val: 0.7456 acc_val: 0.7533 time: 0.0152s\n",
      "Epoch: 0310 loss_train: 0.2975 acc_train: 0.9786 loss_val: 0.7805 acc_val: 0.7533 time: 0.0161s\n",
      "Epoch: 0311 loss_train: 0.3389 acc_train: 0.9571 loss_val: 0.7623 acc_val: 0.7800 time: 0.0155s\n",
      "Epoch: 0312 loss_train: 0.2704 acc_train: 0.9500 loss_val: 0.7338 acc_val: 0.7800 time: 0.0193s\n",
      "Epoch: 0313 loss_train: 0.2935 acc_train: 0.9643 loss_val: 0.7786 acc_val: 0.7900 time: 0.0136s\n",
      "Epoch: 0314 loss_train: 0.3163 acc_train: 0.9357 loss_val: 0.6915 acc_val: 0.7767 time: 0.0166s\n",
      "Epoch: 0315 loss_train: 0.2894 acc_train: 0.9571 loss_val: 0.7143 acc_val: 0.7800 time: 0.0141s\n",
      "Epoch: 0316 loss_train: 0.3092 acc_train: 0.9643 loss_val: 0.7698 acc_val: 0.7800 time: 0.0150s\n",
      "Epoch: 0317 loss_train: 0.2712 acc_train: 0.9500 loss_val: 0.7879 acc_val: 0.7667 time: 0.0157s\n",
      "Epoch: 0318 loss_train: 0.2978 acc_train: 0.9643 loss_val: 0.7607 acc_val: 0.7767 time: 0.0158s\n",
      "Epoch: 0319 loss_train: 0.2899 acc_train: 0.9786 loss_val: 0.7978 acc_val: 0.7633 time: 0.0156s\n",
      "Epoch: 0320 loss_train: 0.2969 acc_train: 0.9571 loss_val: 0.7576 acc_val: 0.7700 time: 0.0152s\n",
      "Epoch: 0321 loss_train: 0.2992 acc_train: 0.9500 loss_val: 0.7227 acc_val: 0.7933 time: 0.0146s\n",
      "Epoch: 0322 loss_train: 0.2781 acc_train: 0.9786 loss_val: 0.7871 acc_val: 0.7267 time: 0.0194s\n",
      "Epoch: 0323 loss_train: 0.2906 acc_train: 0.9714 loss_val: 0.7454 acc_val: 0.7833 time: 0.0132s\n",
      "Epoch: 0324 loss_train: 0.3378 acc_train: 0.9500 loss_val: 0.8052 acc_val: 0.7600 time: 0.0157s\n",
      "Epoch: 0325 loss_train: 0.2741 acc_train: 0.9714 loss_val: 0.7517 acc_val: 0.7767 time: 0.0160s\n",
      "Epoch: 0326 loss_train: 0.2802 acc_train: 0.9643 loss_val: 0.7278 acc_val: 0.7767 time: 0.0192s\n",
      "Epoch: 0327 loss_train: 0.2688 acc_train: 0.9571 loss_val: 0.7173 acc_val: 0.7833 time: 0.0158s\n",
      "Epoch: 0328 loss_train: 0.3164 acc_train: 0.9429 loss_val: 0.7531 acc_val: 0.7800 time: 0.0152s\n",
      "Epoch: 0329 loss_train: 0.2586 acc_train: 0.9714 loss_val: 0.7280 acc_val: 0.7800 time: 0.0163s\n",
      "Epoch: 0330 loss_train: 0.3182 acc_train: 0.9429 loss_val: 0.7473 acc_val: 0.7700 time: 0.0164s\n",
      "Epoch: 0331 loss_train: 0.2566 acc_train: 0.9786 loss_val: 0.7469 acc_val: 0.7633 time: 0.0169s\n",
      "Epoch: 0332 loss_train: 0.2725 acc_train: 0.9643 loss_val: 0.7327 acc_val: 0.8033 time: 0.0169s\n",
      "Epoch: 0333 loss_train: 0.2935 acc_train: 0.9571 loss_val: 0.7273 acc_val: 0.7967 time: 0.0156s\n",
      "Epoch: 0334 loss_train: 0.2412 acc_train: 0.9786 loss_val: 0.7856 acc_val: 0.7767 time: 0.0149s\n",
      "Epoch: 0335 loss_train: 0.3019 acc_train: 0.9714 loss_val: 0.7870 acc_val: 0.8000 time: 0.0147s\n",
      "Epoch: 0336 loss_train: 0.2704 acc_train: 0.9786 loss_val: 0.7437 acc_val: 0.7867 time: 0.0143s\n",
      "Epoch: 0337 loss_train: 0.3119 acc_train: 0.9500 loss_val: 0.7518 acc_val: 0.8067 time: 0.0180s\n",
      "Epoch: 0338 loss_train: 0.2468 acc_train: 0.9571 loss_val: 0.7214 acc_val: 0.7767 time: 0.0171s\n",
      "Epoch: 0339 loss_train: 0.2444 acc_train: 0.9643 loss_val: 0.7316 acc_val: 0.7800 time: 0.0170s\n",
      "Epoch: 0340 loss_train: 0.2758 acc_train: 0.9571 loss_val: 0.7547 acc_val: 0.7833 time: 0.0164s\n",
      "Epoch: 0341 loss_train: 0.2891 acc_train: 0.9714 loss_val: 0.7465 acc_val: 0.7867 time: 0.0169s\n",
      "Epoch: 0342 loss_train: 0.2617 acc_train: 0.9786 loss_val: 0.7707 acc_val: 0.7733 time: 0.0160s\n",
      "Epoch: 0343 loss_train: 0.2542 acc_train: 0.9643 loss_val: 0.7296 acc_val: 0.8067 time: 0.0166s\n",
      "Epoch: 0344 loss_train: 0.3159 acc_train: 0.9357 loss_val: 0.7477 acc_val: 0.7733 time: 0.0171s\n",
      "Epoch: 0345 loss_train: 0.2636 acc_train: 0.9714 loss_val: 0.6937 acc_val: 0.8067 time: 0.0172s\n",
      "Epoch: 0346 loss_train: 0.2611 acc_train: 0.9571 loss_val: 0.7210 acc_val: 0.7733 time: 0.0176s\n",
      "Epoch: 0347 loss_train: 0.2933 acc_train: 0.9714 loss_val: 0.7642 acc_val: 0.7933 time: 0.0152s\n",
      "Epoch: 0348 loss_train: 0.3219 acc_train: 0.9357 loss_val: 0.7387 acc_val: 0.7867 time: 0.0180s\n",
      "Epoch: 0349 loss_train: 0.2462 acc_train: 0.9857 loss_val: 0.7820 acc_val: 0.7900 time: 0.0158s\n",
      "Epoch: 0350 loss_train: 0.2593 acc_train: 0.9786 loss_val: 0.7792 acc_val: 0.7667 time: 0.0163s\n",
      "Epoch: 0351 loss_train: 0.2447 acc_train: 0.9786 loss_val: 0.7372 acc_val: 0.7800 time: 0.0148s\n",
      "Epoch: 0352 loss_train: 0.2976 acc_train: 0.9357 loss_val: 0.7893 acc_val: 0.7733 time: 0.0152s\n",
      "Epoch: 0353 loss_train: 0.2846 acc_train: 0.9714 loss_val: 0.7747 acc_val: 0.7833 time: 0.0153s\n",
      "Epoch: 0354 loss_train: 0.2610 acc_train: 0.9571 loss_val: 0.6901 acc_val: 0.8133 time: 0.0170s\n",
      "Epoch: 0355 loss_train: 0.2845 acc_train: 0.9714 loss_val: 0.7836 acc_val: 0.7833 time: 0.0157s\n",
      "Epoch: 0356 loss_train: 0.2652 acc_train: 0.9714 loss_val: 0.7230 acc_val: 0.7867 time: 0.0162s\n",
      "Epoch: 0357 loss_train: 0.2683 acc_train: 0.9714 loss_val: 0.6900 acc_val: 0.8300 time: 0.0167s\n",
      "Epoch: 0358 loss_train: 0.2980 acc_train: 0.9500 loss_val: 0.6997 acc_val: 0.7933 time: 0.0169s\n",
      "Epoch: 0359 loss_train: 0.2611 acc_train: 0.9643 loss_val: 0.7528 acc_val: 0.7700 time: 0.0202s\n",
      "Epoch: 0360 loss_train: 0.2597 acc_train: 0.9643 loss_val: 0.7124 acc_val: 0.7800 time: 0.0153s\n",
      "Epoch: 0361 loss_train: 0.2839 acc_train: 0.9714 loss_val: 0.7258 acc_val: 0.7833 time: 0.0149s\n",
      "Epoch: 0362 loss_train: 0.2842 acc_train: 0.9643 loss_val: 0.7460 acc_val: 0.7833 time: 0.0156s\n",
      "Epoch: 0363 loss_train: 0.3080 acc_train: 0.9429 loss_val: 0.7805 acc_val: 0.7467 time: 0.0150s\n",
      "Epoch: 0364 loss_train: 0.2749 acc_train: 0.9571 loss_val: 0.7634 acc_val: 0.7800 time: 0.0154s\n",
      "Epoch: 0365 loss_train: 0.2986 acc_train: 0.9786 loss_val: 0.8063 acc_val: 0.7467 time: 0.0159s\n",
      "Epoch: 0366 loss_train: 0.2805 acc_train: 0.9571 loss_val: 0.7530 acc_val: 0.7933 time: 0.0160s\n",
      "Epoch: 0367 loss_train: 0.2869 acc_train: 0.9643 loss_val: 0.7365 acc_val: 0.7800 time: 0.0210s\n",
      "Epoch: 0368 loss_train: 0.2676 acc_train: 0.9571 loss_val: 0.7311 acc_val: 0.7767 time: 0.0143s\n",
      "Epoch: 0369 loss_train: 0.2581 acc_train: 0.9643 loss_val: 0.7202 acc_val: 0.7900 time: 0.0156s\n",
      "Epoch: 0370 loss_train: 0.3010 acc_train: 0.9500 loss_val: 0.7072 acc_val: 0.7800 time: 0.0145s\n",
      "Epoch: 0371 loss_train: 0.2787 acc_train: 0.9714 loss_val: 0.7547 acc_val: 0.7600 time: 0.0160s\n",
      "Epoch: 0372 loss_train: 0.3043 acc_train: 0.9500 loss_val: 0.7743 acc_val: 0.7900 time: 0.0155s\n",
      "Epoch: 0373 loss_train: 0.2489 acc_train: 0.9714 loss_val: 0.7405 acc_val: 0.7900 time: 0.0196s\n",
      "Epoch: 0374 loss_train: 0.2453 acc_train: 0.9929 loss_val: 0.7574 acc_val: 0.7667 time: 0.0157s\n",
      "Epoch: 0375 loss_train: 0.2554 acc_train: 0.9714 loss_val: 0.7443 acc_val: 0.8000 time: 0.0152s\n",
      "Epoch: 0376 loss_train: 0.2779 acc_train: 0.9643 loss_val: 0.7358 acc_val: 0.8067 time: 0.0162s\n",
      "Epoch: 0377 loss_train: 0.2450 acc_train: 0.9857 loss_val: 0.7321 acc_val: 0.7567 time: 0.0154s\n",
      "Epoch: 0378 loss_train: 0.2778 acc_train: 0.9571 loss_val: 0.7445 acc_val: 0.7833 time: 0.0195s\n",
      "Epoch: 0379 loss_train: 0.2579 acc_train: 0.9714 loss_val: 0.7151 acc_val: 0.7833 time: 0.0153s\n",
      "Epoch: 0380 loss_train: 0.2782 acc_train: 0.9643 loss_val: 0.7379 acc_val: 0.7733 time: 0.0158s\n",
      "Epoch: 0381 loss_train: 0.2575 acc_train: 0.9571 loss_val: 0.7313 acc_val: 0.7900 time: 0.0141s\n",
      "Epoch: 0382 loss_train: 0.2417 acc_train: 0.9786 loss_val: 0.7094 acc_val: 0.7833 time: 0.0153s\n",
      "Epoch: 0383 loss_train: 0.2802 acc_train: 0.9429 loss_val: 0.7457 acc_val: 0.7900 time: 0.0152s\n",
      "Epoch: 0384 loss_train: 0.2418 acc_train: 0.9714 loss_val: 0.7013 acc_val: 0.8000 time: 0.0157s\n",
      "Epoch: 0385 loss_train: 0.2594 acc_train: 0.9571 loss_val: 0.7311 acc_val: 0.7733 time: 0.0133s\n",
      "Epoch: 0386 loss_train: 0.3341 acc_train: 0.9714 loss_val: 0.6944 acc_val: 0.8067 time: 0.0179s\n",
      "Epoch: 0387 loss_train: 0.2536 acc_train: 0.9643 loss_val: 0.7401 acc_val: 0.7767 time: 0.0155s\n",
      "Epoch: 0388 loss_train: 0.2262 acc_train: 0.9714 loss_val: 0.7113 acc_val: 0.8100 time: 0.0156s\n",
      "Epoch: 0389 loss_train: 0.2539 acc_train: 0.9786 loss_val: 0.7273 acc_val: 0.7767 time: 0.0155s\n",
      "Epoch: 0390 loss_train: 0.2673 acc_train: 0.9643 loss_val: 0.7148 acc_val: 0.7867 time: 0.0146s\n",
      "Epoch: 0391 loss_train: 0.3111 acc_train: 0.9571 loss_val: 0.7709 acc_val: 0.7833 time: 0.0132s\n",
      "Epoch: 0392 loss_train: 0.2981 acc_train: 0.9429 loss_val: 0.7412 acc_val: 0.8033 time: 0.0159s\n",
      "Epoch: 0393 loss_train: 0.2367 acc_train: 0.9643 loss_val: 0.7617 acc_val: 0.7633 time: 0.0156s\n",
      "Epoch: 0394 loss_train: 0.2689 acc_train: 0.9429 loss_val: 0.7784 acc_val: 0.7800 time: 0.0147s\n",
      "Epoch: 0395 loss_train: 0.2708 acc_train: 0.9571 loss_val: 0.7236 acc_val: 0.7933 time: 0.0164s\n",
      "Epoch: 0396 loss_train: 0.2509 acc_train: 0.9786 loss_val: 0.7217 acc_val: 0.7867 time: 0.0159s\n",
      "Epoch: 0397 loss_train: 0.2692 acc_train: 0.9429 loss_val: 0.7628 acc_val: 0.7733 time: 0.0160s\n",
      "Epoch: 0398 loss_train: 0.2529 acc_train: 0.9714 loss_val: 0.7163 acc_val: 0.7833 time: 0.0132s\n",
      "Epoch: 0399 loss_train: 0.2826 acc_train: 0.9429 loss_val: 0.7373 acc_val: 0.7933 time: 0.0179s\n",
      "Epoch: 0400 loss_train: 0.2725 acc_train: 0.9714 loss_val: 0.7371 acc_val: 0.7900 time: 0.0167s\n",
      "Epoch: 0401 loss_train: 0.2574 acc_train: 0.9786 loss_val: 0.7666 acc_val: 0.7833 time: 0.0158s\n",
      "Epoch: 0402 loss_train: 0.2540 acc_train: 0.9786 loss_val: 0.7698 acc_val: 0.7533 time: 0.0159s\n",
      "Epoch: 0403 loss_train: 0.2332 acc_train: 0.9857 loss_val: 0.6946 acc_val: 0.7800 time: 0.0158s\n",
      "Epoch: 0404 loss_train: 0.2775 acc_train: 0.9643 loss_val: 0.7903 acc_val: 0.7567 time: 0.0160s\n",
      "Epoch: 0405 loss_train: 0.2458 acc_train: 0.9786 loss_val: 0.7360 acc_val: 0.7633 time: 0.0173s\n",
      "Epoch: 0406 loss_train: 0.2669 acc_train: 0.9714 loss_val: 0.7537 acc_val: 0.7867 time: 0.0155s\n",
      "Epoch: 0407 loss_train: 0.2643 acc_train: 0.9643 loss_val: 0.7304 acc_val: 0.7700 time: 0.0155s\n",
      "Epoch: 0408 loss_train: 0.2555 acc_train: 0.9643 loss_val: 0.7662 acc_val: 0.7767 time: 0.0148s\n",
      "Epoch: 0409 loss_train: 0.2874 acc_train: 0.9357 loss_val: 0.7035 acc_val: 0.7800 time: 0.0141s\n",
      "Epoch: 0410 loss_train: 0.2499 acc_train: 0.9714 loss_val: 0.7207 acc_val: 0.7767 time: 0.0154s\n",
      "Epoch: 0411 loss_train: 0.2916 acc_train: 0.9571 loss_val: 0.7279 acc_val: 0.7833 time: 0.0153s\n",
      "Epoch: 0412 loss_train: 0.2416 acc_train: 0.9786 loss_val: 0.7449 acc_val: 0.7567 time: 0.0215s\n",
      "Epoch: 0413 loss_train: 0.2495 acc_train: 0.9643 loss_val: 0.7947 acc_val: 0.7400 time: 0.0165s\n",
      "Epoch: 0414 loss_train: 0.2694 acc_train: 0.9571 loss_val: 0.7296 acc_val: 0.7833 time: 0.0195s\n",
      "Epoch: 0415 loss_train: 0.2544 acc_train: 0.9714 loss_val: 0.7309 acc_val: 0.7667 time: 0.0161s\n",
      "Epoch: 0416 loss_train: 0.2300 acc_train: 0.9643 loss_val: 0.7108 acc_val: 0.7800 time: 0.0171s\n",
      "Epoch: 0417 loss_train: 0.2713 acc_train: 0.9714 loss_val: 0.7766 acc_val: 0.7600 time: 0.0159s\n",
      "Epoch: 0418 loss_train: 0.2804 acc_train: 0.9571 loss_val: 0.8163 acc_val: 0.7633 time: 0.0145s\n",
      "Epoch: 0419 loss_train: 0.2715 acc_train: 0.9500 loss_val: 0.7401 acc_val: 0.7967 time: 0.0200s\n",
      "Epoch: 0420 loss_train: 0.2301 acc_train: 0.9857 loss_val: 0.7205 acc_val: 0.7667 time: 0.0159s\n",
      "Epoch: 0421 loss_train: 0.2446 acc_train: 0.9571 loss_val: 0.7648 acc_val: 0.7633 time: 0.0146s\n",
      "Epoch: 0422 loss_train: 0.2750 acc_train: 0.9714 loss_val: 0.7727 acc_val: 0.7700 time: 0.0148s\n",
      "Epoch: 0423 loss_train: 0.2313 acc_train: 0.9643 loss_val: 0.7079 acc_val: 0.7867 time: 0.0145s\n",
      "Epoch: 0424 loss_train: 0.2385 acc_train: 0.9786 loss_val: 0.7144 acc_val: 0.7900 time: 0.0184s\n",
      "Epoch: 0425 loss_train: 0.2645 acc_train: 0.9786 loss_val: 0.7703 acc_val: 0.7833 time: 0.0147s\n",
      "Epoch: 0426 loss_train: 0.2737 acc_train: 0.9786 loss_val: 0.7601 acc_val: 0.7767 time: 0.0153s\n",
      "Epoch: 0427 loss_train: 0.2701 acc_train: 0.9500 loss_val: 0.7037 acc_val: 0.8033 time: 0.0150s\n",
      "Epoch: 0428 loss_train: 0.2574 acc_train: 0.9500 loss_val: 0.7438 acc_val: 0.7833 time: 0.0157s\n",
      "Epoch: 0429 loss_train: 0.2603 acc_train: 0.9714 loss_val: 0.7939 acc_val: 0.7733 time: 0.0158s\n",
      "Epoch: 0430 loss_train: 0.2472 acc_train: 0.9857 loss_val: 0.7549 acc_val: 0.7533 time: 0.0179s\n",
      "Epoch: 0431 loss_train: 0.2404 acc_train: 0.9643 loss_val: 0.7473 acc_val: 0.7633 time: 0.0210s\n",
      "Epoch: 0432 loss_train: 0.2429 acc_train: 0.9714 loss_val: 0.7355 acc_val: 0.7867 time: 0.0165s\n",
      "Epoch: 0433 loss_train: 0.2691 acc_train: 0.9714 loss_val: 0.7209 acc_val: 0.7700 time: 0.0155s\n",
      "Epoch: 0434 loss_train: 0.2652 acc_train: 0.9714 loss_val: 0.7407 acc_val: 0.7767 time: 0.0160s\n",
      "Epoch: 0435 loss_train: 0.2589 acc_train: 0.9571 loss_val: 0.7323 acc_val: 0.7833 time: 0.0164s\n",
      "Epoch: 0436 loss_train: 0.2561 acc_train: 0.9786 loss_val: 0.7197 acc_val: 0.7967 time: 0.0148s\n",
      "Epoch: 0437 loss_train: 0.2732 acc_train: 0.9429 loss_val: 0.6729 acc_val: 0.7867 time: 0.0198s\n",
      "Epoch: 0438 loss_train: 0.2341 acc_train: 0.9786 loss_val: 0.7517 acc_val: 0.7967 time: 0.0135s\n",
      "Epoch: 0439 loss_train: 0.2285 acc_train: 0.9643 loss_val: 0.6854 acc_val: 0.7967 time: 0.0134s\n",
      "Epoch: 0440 loss_train: 0.2567 acc_train: 0.9714 loss_val: 0.7314 acc_val: 0.7700 time: 0.0133s\n",
      "Epoch: 0441 loss_train: 0.2570 acc_train: 0.9643 loss_val: 0.7920 acc_val: 0.7700 time: 0.0148s\n",
      "Epoch: 0442 loss_train: 0.2434 acc_train: 0.9857 loss_val: 0.7561 acc_val: 0.8067 time: 0.0148s\n",
      "Epoch: 0443 loss_train: 0.2368 acc_train: 0.9571 loss_val: 0.7079 acc_val: 0.7833 time: 0.0142s\n",
      "Epoch: 0444 loss_train: 0.2757 acc_train: 0.9571 loss_val: 0.7460 acc_val: 0.7900 time: 0.0165s\n",
      "Epoch: 0445 loss_train: 0.2295 acc_train: 0.9786 loss_val: 0.6997 acc_val: 0.7767 time: 0.0165s\n",
      "Epoch: 0446 loss_train: 0.2185 acc_train: 0.9786 loss_val: 0.7792 acc_val: 0.7867 time: 0.0166s\n",
      "Epoch: 0447 loss_train: 0.2518 acc_train: 0.9786 loss_val: 0.7397 acc_val: 0.7667 time: 0.0171s\n",
      "Epoch: 0448 loss_train: 0.2437 acc_train: 0.9643 loss_val: 0.6907 acc_val: 0.7933 time: 0.0177s\n",
      "Epoch: 0449 loss_train: 0.2188 acc_train: 0.9643 loss_val: 0.6837 acc_val: 0.7867 time: 0.0183s\n",
      "Epoch: 0450 loss_train: 0.2609 acc_train: 0.9714 loss_val: 0.7743 acc_val: 0.7633 time: 0.0150s\n",
      "Epoch: 0451 loss_train: 0.2672 acc_train: 0.9643 loss_val: 0.7490 acc_val: 0.7967 time: 0.0174s\n",
      "Epoch: 0452 loss_train: 0.2160 acc_train: 0.9929 loss_val: 0.7157 acc_val: 0.8033 time: 0.0159s\n",
      "Epoch: 0453 loss_train: 0.2218 acc_train: 0.9929 loss_val: 0.7819 acc_val: 0.7400 time: 0.0175s\n",
      "Epoch: 0454 loss_train: 0.2580 acc_train: 0.9571 loss_val: 0.7228 acc_val: 0.7933 time: 0.0187s\n",
      "Epoch: 0455 loss_train: 0.2236 acc_train: 0.9714 loss_val: 0.7305 acc_val: 0.7733 time: 0.0192s\n",
      "Epoch: 0456 loss_train: 0.2622 acc_train: 0.9643 loss_val: 0.7770 acc_val: 0.7500 time: 0.0171s\n",
      "Epoch: 0457 loss_train: 0.2344 acc_train: 0.9643 loss_val: 0.6745 acc_val: 0.8033 time: 0.0151s\n",
      "Epoch: 0458 loss_train: 0.2370 acc_train: 0.9786 loss_val: 0.7439 acc_val: 0.7767 time: 0.0135s\n",
      "Epoch: 0459 loss_train: 0.2423 acc_train: 0.9929 loss_val: 0.7289 acc_val: 0.7567 time: 0.0147s\n",
      "Epoch: 0460 loss_train: 0.2403 acc_train: 0.9714 loss_val: 0.7078 acc_val: 0.7833 time: 0.0149s\n",
      "Epoch: 0461 loss_train: 0.2338 acc_train: 0.9643 loss_val: 0.6993 acc_val: 0.7867 time: 0.0176s\n",
      "Epoch: 0462 loss_train: 0.2317 acc_train: 0.9786 loss_val: 0.7676 acc_val: 0.7600 time: 0.0158s\n",
      "Epoch: 0463 loss_train: 0.2525 acc_train: 0.9643 loss_val: 0.7433 acc_val: 0.7800 time: 0.0172s\n",
      "Epoch: 0464 loss_train: 0.2461 acc_train: 0.9714 loss_val: 0.7410 acc_val: 0.7900 time: 0.0141s\n",
      "Epoch: 0465 loss_train: 0.2451 acc_train: 0.9643 loss_val: 0.7101 acc_val: 0.7700 time: 0.0142s\n",
      "Epoch: 0466 loss_train: 0.2663 acc_train: 0.9429 loss_val: 0.7270 acc_val: 0.7833 time: 0.0185s\n",
      "Epoch: 0467 loss_train: 0.2773 acc_train: 0.9571 loss_val: 0.7375 acc_val: 0.7867 time: 0.0147s\n",
      "Epoch: 0468 loss_train: 0.2380 acc_train: 0.9786 loss_val: 0.7213 acc_val: 0.7767 time: 0.0149s\n",
      "Epoch: 0469 loss_train: 0.2658 acc_train: 0.9643 loss_val: 0.7030 acc_val: 0.7767 time: 0.0169s\n",
      "Epoch: 0470 loss_train: 0.2407 acc_train: 0.9714 loss_val: 0.7206 acc_val: 0.7900 time: 0.0174s\n",
      "Epoch: 0471 loss_train: 0.2387 acc_train: 0.9571 loss_val: 0.7463 acc_val: 0.7833 time: 0.0142s\n",
      "Epoch: 0472 loss_train: 0.2368 acc_train: 0.9714 loss_val: 0.7026 acc_val: 0.7633 time: 0.0147s\n",
      "Epoch: 0473 loss_train: 0.2271 acc_train: 0.9786 loss_val: 0.7180 acc_val: 0.7667 time: 0.0180s\n",
      "Epoch: 0474 loss_train: 0.2519 acc_train: 0.9786 loss_val: 0.7532 acc_val: 0.7867 time: 0.0147s\n",
      "Epoch: 0475 loss_train: 0.2632 acc_train: 0.9643 loss_val: 0.7545 acc_val: 0.7633 time: 0.0162s\n",
      "Epoch: 0476 loss_train: 0.2215 acc_train: 0.9857 loss_val: 0.7351 acc_val: 0.8000 time: 0.0160s\n",
      "Epoch: 0477 loss_train: 0.2311 acc_train: 0.9786 loss_val: 0.6724 acc_val: 0.8000 time: 0.0162s\n",
      "Epoch: 0478 loss_train: 0.2207 acc_train: 0.9714 loss_val: 0.7197 acc_val: 0.7867 time: 0.0202s\n",
      "Epoch: 0479 loss_train: 0.2553 acc_train: 0.9643 loss_val: 0.7806 acc_val: 0.7867 time: 0.0181s\n",
      "Epoch: 0480 loss_train: 0.2343 acc_train: 0.9786 loss_val: 0.7466 acc_val: 0.7700 time: 0.0141s\n",
      "Epoch: 0481 loss_train: 0.2237 acc_train: 0.9786 loss_val: 0.7621 acc_val: 0.7800 time: 0.0159s\n",
      "Epoch: 0482 loss_train: 0.2072 acc_train: 0.9929 loss_val: 0.6665 acc_val: 0.8067 time: 0.0171s\n",
      "Epoch: 0483 loss_train: 0.2492 acc_train: 0.9500 loss_val: 0.7510 acc_val: 0.7667 time: 0.0152s\n",
      "Epoch: 0484 loss_train: 0.2326 acc_train: 0.9929 loss_val: 0.7411 acc_val: 0.7667 time: 0.0171s\n",
      "Epoch: 0485 loss_train: 0.2215 acc_train: 0.9857 loss_val: 0.7662 acc_val: 0.7567 time: 0.0172s\n",
      "Epoch: 0486 loss_train: 0.2644 acc_train: 0.9714 loss_val: 0.7330 acc_val: 0.7800 time: 0.0151s\n",
      "Epoch: 0487 loss_train: 0.2291 acc_train: 0.9857 loss_val: 0.7186 acc_val: 0.7900 time: 0.0153s\n",
      "Epoch: 0488 loss_train: 0.2635 acc_train: 0.9500 loss_val: 0.7639 acc_val: 0.7433 time: 0.0145s\n",
      "Epoch: 0489 loss_train: 0.2264 acc_train: 0.9714 loss_val: 0.7174 acc_val: 0.7767 time: 0.0169s\n",
      "Epoch: 0490 loss_train: 0.2247 acc_train: 0.9857 loss_val: 0.7268 acc_val: 0.8000 time: 0.0155s\n",
      "Epoch: 0491 loss_train: 0.2357 acc_train: 0.9786 loss_val: 0.7291 acc_val: 0.7867 time: 0.0145s\n",
      "Epoch: 0492 loss_train: 0.2536 acc_train: 0.9571 loss_val: 0.7104 acc_val: 0.7667 time: 0.0148s\n",
      "Epoch: 0493 loss_train: 0.2166 acc_train: 0.9786 loss_val: 0.7286 acc_val: 0.7900 time: 0.0155s\n",
      "Epoch: 0494 loss_train: 0.2508 acc_train: 0.9571 loss_val: 0.7112 acc_val: 0.7800 time: 0.0142s\n",
      "Epoch: 0495 loss_train: 0.2320 acc_train: 0.9714 loss_val: 0.6897 acc_val: 0.7967 time: 0.0141s\n",
      "Epoch: 0496 loss_train: 0.2543 acc_train: 0.9500 loss_val: 0.7006 acc_val: 0.7967 time: 0.0159s\n",
      "Epoch: 0497 loss_train: 0.2511 acc_train: 0.9786 loss_val: 0.7178 acc_val: 0.8033 time: 0.0198s\n",
      "Epoch: 0498 loss_train: 0.2468 acc_train: 0.9857 loss_val: 0.7424 acc_val: 0.7967 time: 0.0189s\n",
      "Epoch: 0499 loss_train: 0.2165 acc_train: 0.9643 loss_val: 0.7045 acc_val: 0.7567 time: 0.0145s\n",
      "Epoch: 0500 loss_train: 0.2324 acc_train: 0.9857 loss_val: 0.7316 acc_val: 0.7667 time: 0.0173s\n",
      "Test set results: loss= 0.6183 accuracy= 0.8290\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)                     # features:(2708, 1433)   adj:(2708, 2708)\n",
    "    loss_test = torch.nn.functional.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMSjazMgQ7ZZ6S2L4g18XiW",
   "collapsed_sections": [],
   "name": "GCN_Full.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
